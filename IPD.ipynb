{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUbH9RHXjAm5ri+OIZl/KD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SafaSaifudeen/Medical_ASR/blob/ipd/IPD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp7WByl46z9g",
        "outputId": "228c3a79-eb0b-4f73-becf-bc42017ae5e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting comet-ml\n",
            "  Downloading comet_ml-3.49.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet-ml)\n",
            "  Downloading dulwich-0.22.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet-ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (4.23.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet-ml)\n",
            "  Downloading python_box-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (2.32.3)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (13.9.4)\n",
            "Collecting semantic-version>=2.8.0 (from comet-ml)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (2.22.0)\n",
            "Collecting simplejson (from comet-ml)\n",
            "  Downloading simplejson-3.20.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (1.17.2)\n",
            "Collecting wurlitzer>=1.0.2 (from comet-ml)\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet-ml)\n",
            "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet-ml) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet-ml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet-ml) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet-ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet-ml) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comet_ml-3.49.3-py3-none-any.whl (719 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.0/720.0 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-0.22.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (979 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m979.7/979.7 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading python_box-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading simplejson-3.20.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: everett, wurlitzer, simplejson, semantic-version, python-box, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dulwich, configobj, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, comet-ml\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.3.2\n",
            "    Uninstalling python-box-7.3.2:\n",
            "      Successfully uninstalled python-box-7.3.2\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed comet-ml-3.49.3 configobj-5.0.9 dulwich-0.22.7 everett-3.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 python-box-6.1.0 semantic-version-2.10.0 simplejson-3.20.1 wurlitzer-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchaudio torch comet-ml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from comet_ml import Experiment\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "\n",
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "\n",
        "def _levenshtein_distance(ref, hyp):\n",
        "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
        "    between two sequences. Informally, the levenshtein disctance is defined as\n",
        "    the minimum number of single-character edits (substitutions, insertions or\n",
        "    deletions) required to change one word into the other. We can naturally\n",
        "    extend the edits to word level when calculate levenshtein disctance for\n",
        "    two sentences.\n",
        "    \"\"\"\n",
        "    m = len(ref)\n",
        "    n = len(hyp)\n",
        "\n",
        "    # special case\n",
        "    if ref == hyp:\n",
        "        return 0\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if m < n:\n",
        "        ref, hyp = hyp, ref\n",
        "        m, n = n, m\n",
        "\n",
        "    # use O(min(m, n)) space\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
        "\n",
        "    # initialize distance matrix\n",
        "    for j in range(0,n + 1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # calculate levenshtein distance\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row_idx = (i - 1) % 2\n",
        "        cur_row_idx = i % 2\n",
        "        distance[cur_row_idx][0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "            else:\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\n",
        "                d_num = distance[prev_row_idx][j] + 1\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "    return distance[m % 2][n]\n",
        "\n",
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in word-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Levenshtein distance and word number of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)\n",
        "\n",
        "\n",
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in char-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Levenshtein distance and length of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)\n",
        "\n",
        "\n",
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
        "    hypothesis text in word-level. WER is defined as:\n",
        "    .. math::\n",
        "        WER = (Sw + Dw + Iw) / Nw\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sw is the number of words subsituted,\n",
        "        Dw is the number of words deleted,\n",
        "        Iw is the number of words inserted,\n",
        "        Nw is the number of words in the reference\n",
        "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
        "    that empty items will be removed when splitting sentences by delimiter.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Word error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If word number of reference is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
        "                                         delimiter)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer\n",
        "\n",
        "\n",
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
        "    hypothesis text in char-level. CER is defined as:\n",
        "    .. math::\n",
        "        CER = (Sc + Dc + Ic) / Nc\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sc is the number of characters substituted,\n",
        "        Dc is the number of characters deleted,\n",
        "        Ic is the number of characters inserted\n",
        "        Nc is the number of characters in the reference\n",
        "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
        "    encoded to unicode. Please draw an attention that the leading and tailing\n",
        "    space characters will be truncated and multiple consecutive space\n",
        "    characters in a sentence will be replaced by one space character.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Character error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If the reference length is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        ' 0\n",
        "        <SPACE> 1\n",
        "        a 2\n",
        "        b 3\n",
        "        c 4\n",
        "        d 5\n",
        "        e 6\n",
        "        f 7\n",
        "        g 8\n",
        "        h 9\n",
        "        i 10\n",
        "        j 11\n",
        "        k 12\n",
        "        l 13\n",
        "        m 14\n",
        "        n 15\n",
        "        o 16\n",
        "        p 17\n",
        "        q 18\n",
        "        r 19\n",
        "        s 20\n",
        "        t 21\n",
        "        u 22\n",
        "        v 23\n",
        "        w 24\n",
        "        x 25\n",
        "        y 26\n",
        "        z 27\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')\n",
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tol8WfTg8Eh5",
        "outputId": "3f866929-11dc-457f-a02e-d86241b6728e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**\n",
        "\n",
        "Base of of Deep Speech 2"
      ],
      "metadata": {
        "id": "hWR9p7E98TfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "uxWgSIiN8EkL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train**"
      ],
      "metadata": {
        "id": "kpeD4Qsy8efF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    with experiment.train():\n",
        "        for batch_idx, _data in enumerate(train_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            loss.backward()\n",
        "\n",
        "            experiment.log_metric('loss', loss.item(), step=iter_meter.get())\n",
        "            experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            iter_meter.step()\n",
        "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(spectrograms), data_len,\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with experiment.test():\n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(test_loader):\n",
        "                spectrograms, labels, input_lengths, label_lengths = _data\n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "                for j in range(len(decoded_preds)):\n",
        "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())\n",
        "    experiment.log_metric('cer', avg_cer, step=iter_meter.get())\n",
        "    experiment.log_metric('wer', avg_wer, step=iter_meter.get())\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "def main(learning_rate=5e-4, batch_size=20, epochs=10,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\",\n",
        "        experiment=Experiment(api_key='dummy_key', disabled=True)):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 29,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    experiment.log_parameters(hparams)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "    print(model)\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=28).to(device)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "\n",
        "    iter_meter = IterMeter()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\n",
        "        test(model, device, test_loader, criterion, epoch, iter_meter, experiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjmg1cbE8Emt",
        "outputId": "e7d19c8c-f2c0-4088-b88b-494657450b71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up Comet**"
      ],
      "metadata": {
        "id": "_w67IgDj8xam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comet_api_key = \"VfumPp4GJIPw6x7dYo6ABHyAU\"\n",
        "project_name = \"speechrecognition\"\n",
        "experiment_name = \"speechrecognition-colab\"\n",
        "\n",
        "if comet_api_key:\n",
        "  experiment = Experiment(api_key=comet_api_key, project_name=project_name, parse_args=False)\n",
        "  experiment.set_name(experiment_name)\n",
        "  experiment.display()\n",
        "else:\n",
        "  experiment = Experiment(api_key='dummy_key', disabled=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "XiDG33cF8EpP",
        "outputId": "c3e1fdda-cda9-425f-f811-87b00cd917fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/safa-saifudeen/speechrecognition/65aff2a8b0f24694898620904db6fdc4\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7bfc9330cd10>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"800px\"\n",
              "            src=\"https://www.comet.com/safa-saifudeen/speechrecognition/65aff2a8b0f24694898620904db6fdc4\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfIhv9yE9TU-",
        "outputId": "4cfc7ef1-a9c7-49b0-a006-c53b56989f5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Feb 28 14:28:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set, experiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6r-wANL8Erw",
        "outputId": "09efeb73-c1eb-42f1-9d5d-afd19c97010b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.95G/5.95G [03:06<00:00, 34.2MB/s]\n",
            "100%|██████████| 331M/331M [00:11<00:00, 31.3MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SpeechRecognitionModel(\n",
            "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (rescnn_layers): Sequential(\n",
            "    (0): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (2): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (birnn_layers): Sequential(\n",
            "    (0): BidirectionalGRU(\n",
            "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (2): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (3): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (4): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): GELU(approximate='none')\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
            "  )\n",
            ")\n",
            "Num Model Parameters 23705373\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:2114: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  _warn_get_lr_called_within_step(self)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/28539 (0%)]\tLoss: 6.524758\n",
            "Train Epoch: 1 [1000/28539 (4%)]\tLoss: 2.951037\n",
            "Train Epoch: 1 [2000/28539 (7%)]\tLoss: 2.907643\n",
            "Train Epoch: 1 [3000/28539 (11%)]\tLoss: 2.861320\n",
            "Train Epoch: 1 [4000/28539 (14%)]\tLoss: 2.876525\n",
            "Train Epoch: 1 [5000/28539 (18%)]\tLoss: 2.886673\n",
            "Train Epoch: 1 [6000/28539 (21%)]\tLoss: 2.858836\n",
            "Train Epoch: 1 [7000/28539 (25%)]\tLoss: 2.846978\n",
            "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 2.953497\n",
            "Train Epoch: 1 [9000/28539 (32%)]\tLoss: 2.877760\n",
            "Train Epoch: 1 [10000/28539 (35%)]\tLoss: 2.889043\n",
            "Train Epoch: 1 [11000/28539 (39%)]\tLoss: 2.897018\n",
            "Train Epoch: 1 [12000/28539 (42%)]\tLoss: 2.868230\n",
            "Train Epoch: 1 [13000/28539 (46%)]\tLoss: 2.828988\n",
            "Train Epoch: 1 [14000/28539 (49%)]\tLoss: 2.836649\n",
            "Train Epoch: 1 [15000/28539 (53%)]\tLoss: 2.796366\n",
            "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 2.697027\n",
            "Train Epoch: 1 [17000/28539 (60%)]\tLoss: 2.547825\n",
            "Train Epoch: 1 [18000/28539 (63%)]\tLoss: 2.317186\n",
            "Train Epoch: 1 [19000/28539 (67%)]\tLoss: 2.181663\n",
            "Train Epoch: 1 [20000/28539 (70%)]\tLoss: 2.152337\n",
            "Train Epoch: 1 [21000/28539 (74%)]\tLoss: 2.152335\n",
            "Train Epoch: 1 [22000/28539 (77%)]\tLoss: 1.872695\n",
            "Train Epoch: 1 [23000/28539 (81%)]\tLoss: 1.888399\n",
            "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 1.881756\n",
            "Train Epoch: 1 [25000/28539 (88%)]\tLoss: 1.792916\n",
            "Train Epoch: 1 [26000/28539 (91%)]\tLoss: 1.610741\n",
            "Train Epoch: 1 [27000/28539 (95%)]\tLoss: 1.647141\n",
            "Train Epoch: 1 [28000/28539 (98%)]\tLoss: 1.666828\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.4878, Average CER: 0.442533 Average WER: 0.9912\n",
            "\n",
            "Train Epoch: 2 [0/28539 (0%)]\tLoss: 1.794197\n",
            "Train Epoch: 2 [1000/28539 (4%)]\tLoss: 1.553072\n",
            "Train Epoch: 2 [2000/28539 (7%)]\tLoss: 1.467368\n",
            "Train Epoch: 2 [3000/28539 (11%)]\tLoss: 1.591259\n",
            "Train Epoch: 2 [4000/28539 (14%)]\tLoss: 1.680864\n",
            "Train Epoch: 2 [5000/28539 (18%)]\tLoss: 1.621847\n",
            "Train Epoch: 2 [6000/28539 (21%)]\tLoss: 1.438600\n",
            "Train Epoch: 2 [7000/28539 (25%)]\tLoss: 1.431296\n",
            "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 1.552179\n",
            "Train Epoch: 2 [9000/28539 (32%)]\tLoss: 1.392695\n",
            "Train Epoch: 2 [10000/28539 (35%)]\tLoss: 1.395192\n",
            "Train Epoch: 2 [11000/28539 (39%)]\tLoss: 1.416663\n",
            "Train Epoch: 2 [12000/28539 (42%)]\tLoss: 1.385052\n",
            "Train Epoch: 2 [13000/28539 (46%)]\tLoss: 1.381687\n",
            "Train Epoch: 2 [14000/28539 (49%)]\tLoss: 1.280750\n",
            "Train Epoch: 2 [15000/28539 (53%)]\tLoss: 1.403559\n",
            "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 1.404078\n",
            "Train Epoch: 2 [17000/28539 (60%)]\tLoss: 1.356666\n",
            "Train Epoch: 2 [18000/28539 (63%)]\tLoss: 1.265602\n",
            "Train Epoch: 2 [19000/28539 (67%)]\tLoss: 1.377079\n",
            "Train Epoch: 2 [20000/28539 (70%)]\tLoss: 1.361424\n",
            "Train Epoch: 2 [21000/28539 (74%)]\tLoss: 1.383349\n",
            "Train Epoch: 2 [22000/28539 (77%)]\tLoss: 1.188803\n",
            "Train Epoch: 2 [23000/28539 (81%)]\tLoss: 1.262598\n",
            "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 1.201997\n",
            "Train Epoch: 2 [25000/28539 (88%)]\tLoss: 1.169734\n",
            "Train Epoch: 2 [26000/28539 (91%)]\tLoss: 1.083020\n",
            "Train Epoch: 2 [27000/28539 (95%)]\tLoss: 1.340185\n",
            "Train Epoch: 2 [28000/28539 (98%)]\tLoss: 1.307969\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.0311, Average CER: 0.312753 Average WER: 0.7835\n",
            "\n",
            "Train Epoch: 3 [0/28539 (0%)]\tLoss: 1.060181\n",
            "Train Epoch: 3 [1000/28539 (4%)]\tLoss: 1.288507\n",
            "Train Epoch: 3 [2000/28539 (7%)]\tLoss: 1.122103\n",
            "Train Epoch: 3 [3000/28539 (11%)]\tLoss: 1.152910\n",
            "Train Epoch: 3 [4000/28539 (14%)]\tLoss: 1.270094\n",
            "Train Epoch: 3 [5000/28539 (18%)]\tLoss: 1.144840\n",
            "Train Epoch: 3 [6000/28539 (21%)]\tLoss: 1.272664\n",
            "Train Epoch: 3 [7000/28539 (25%)]\tLoss: 1.156170\n",
            "Train Epoch: 3 [8000/28539 (28%)]\tLoss: 1.140670\n",
            "Train Epoch: 3 [9000/28539 (32%)]\tLoss: 1.191120\n",
            "Train Epoch: 3 [10000/28539 (35%)]\tLoss: 1.284840\n",
            "Train Epoch: 3 [11000/28539 (39%)]\tLoss: 1.167481\n",
            "Train Epoch: 3 [12000/28539 (42%)]\tLoss: 1.169700\n",
            "Train Epoch: 3 [13000/28539 (46%)]\tLoss: 1.184177\n",
            "Train Epoch: 3 [14000/28539 (49%)]\tLoss: 0.990601\n",
            "Train Epoch: 3 [15000/28539 (53%)]\tLoss: 1.108223\n",
            "Train Epoch: 3 [16000/28539 (56%)]\tLoss: 1.075774\n",
            "Train Epoch: 3 [17000/28539 (60%)]\tLoss: 0.958158\n",
            "Train Epoch: 3 [18000/28539 (63%)]\tLoss: 1.078093\n",
            "Train Epoch: 3 [19000/28539 (67%)]\tLoss: 1.094654\n",
            "Train Epoch: 3 [20000/28539 (70%)]\tLoss: 1.056795\n",
            "Train Epoch: 3 [21000/28539 (74%)]\tLoss: 0.988490\n",
            "Train Epoch: 3 [22000/28539 (77%)]\tLoss: 1.288363\n",
            "Train Epoch: 3 [23000/28539 (81%)]\tLoss: 0.993847\n",
            "Train Epoch: 3 [24000/28539 (84%)]\tLoss: 1.152728\n",
            "Train Epoch: 3 [25000/28539 (88%)]\tLoss: 1.177871\n",
            "Train Epoch: 3 [26000/28539 (91%)]\tLoss: 1.100961\n",
            "Train Epoch: 3 [27000/28539 (95%)]\tLoss: 1.032592\n",
            "Train Epoch: 3 [28000/28539 (98%)]\tLoss: 1.073193\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.8507, Average CER: 0.261900 Average WER: 0.7008\n",
            "\n",
            "Train Epoch: 4 [0/28539 (0%)]\tLoss: 1.008013\n",
            "Train Epoch: 4 [1000/28539 (4%)]\tLoss: 1.037006\n",
            "Train Epoch: 4 [2000/28539 (7%)]\tLoss: 1.092662\n",
            "Train Epoch: 4 [3000/28539 (11%)]\tLoss: 1.292734\n",
            "Train Epoch: 4 [4000/28539 (14%)]\tLoss: 0.900669\n",
            "Train Epoch: 4 [5000/28539 (18%)]\tLoss: 0.901987\n",
            "Train Epoch: 4 [6000/28539 (21%)]\tLoss: 0.953827\n",
            "Train Epoch: 4 [7000/28539 (25%)]\tLoss: 0.855419\n",
            "Train Epoch: 4 [8000/28539 (28%)]\tLoss: 0.923470\n",
            "Train Epoch: 4 [9000/28539 (32%)]\tLoss: 0.925518\n",
            "Train Epoch: 4 [10000/28539 (35%)]\tLoss: 0.899854\n",
            "Train Epoch: 4 [11000/28539 (39%)]\tLoss: 0.939590\n",
            "Train Epoch: 4 [12000/28539 (42%)]\tLoss: 0.933596\n",
            "Train Epoch: 4 [13000/28539 (46%)]\tLoss: 0.960715\n",
            "Train Epoch: 4 [14000/28539 (49%)]\tLoss: 0.921186\n",
            "Train Epoch: 4 [15000/28539 (53%)]\tLoss: 1.212160\n",
            "Train Epoch: 4 [16000/28539 (56%)]\tLoss: 1.214565\n",
            "Train Epoch: 4 [17000/28539 (60%)]\tLoss: 1.234590\n",
            "Train Epoch: 4 [18000/28539 (63%)]\tLoss: 1.002930\n",
            "Train Epoch: 4 [19000/28539 (67%)]\tLoss: 1.101193\n",
            "Train Epoch: 4 [20000/28539 (70%)]\tLoss: 0.800853\n",
            "Train Epoch: 4 [21000/28539 (74%)]\tLoss: 1.139188\n",
            "Train Epoch: 4 [22000/28539 (77%)]\tLoss: 0.908580\n",
            "Train Epoch: 4 [23000/28539 (81%)]\tLoss: 0.923282\n",
            "Train Epoch: 4 [24000/28539 (84%)]\tLoss: 0.949559\n",
            "Train Epoch: 4 [25000/28539 (88%)]\tLoss: 0.832700\n",
            "Train Epoch: 4 [26000/28539 (91%)]\tLoss: 1.053717\n",
            "Train Epoch: 4 [27000/28539 (95%)]\tLoss: 1.029488\n",
            "Train Epoch: 4 [28000/28539 (98%)]\tLoss: 0.920517\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.7582, Average CER: 0.229627 Average WER: 0.6386\n",
            "\n",
            "Train Epoch: 5 [0/28539 (0%)]\tLoss: 0.918448\n",
            "Train Epoch: 5 [1000/28539 (4%)]\tLoss: 0.779598\n",
            "Train Epoch: 5 [2000/28539 (7%)]\tLoss: 0.839252\n",
            "Train Epoch: 5 [3000/28539 (11%)]\tLoss: 1.024132\n",
            "Train Epoch: 5 [4000/28539 (14%)]\tLoss: 0.815065\n",
            "Train Epoch: 5 [5000/28539 (18%)]\tLoss: 0.941661\n",
            "Train Epoch: 5 [6000/28539 (21%)]\tLoss: 1.023820\n",
            "Train Epoch: 5 [7000/28539 (25%)]\tLoss: 0.882822\n",
            "Train Epoch: 5 [8000/28539 (28%)]\tLoss: 0.963446\n",
            "Train Epoch: 5 [9000/28539 (32%)]\tLoss: 1.021792\n",
            "Train Epoch: 5 [10000/28539 (35%)]\tLoss: 0.837864\n",
            "Train Epoch: 5 [11000/28539 (39%)]\tLoss: 0.826182\n",
            "Train Epoch: 5 [12000/28539 (42%)]\tLoss: 0.902786\n",
            "Train Epoch: 5 [13000/28539 (46%)]\tLoss: 0.827969\n",
            "Train Epoch: 5 [14000/28539 (49%)]\tLoss: 0.776457\n",
            "Train Epoch: 5 [15000/28539 (53%)]\tLoss: 1.005978\n",
            "Train Epoch: 5 [16000/28539 (56%)]\tLoss: 1.000666\n",
            "Train Epoch: 5 [17000/28539 (60%)]\tLoss: 0.860322\n",
            "Train Epoch: 5 [18000/28539 (63%)]\tLoss: 0.872254\n",
            "Train Epoch: 5 [19000/28539 (67%)]\tLoss: 0.993204\n",
            "Train Epoch: 5 [20000/28539 (70%)]\tLoss: 0.931751\n",
            "Train Epoch: 5 [21000/28539 (74%)]\tLoss: 0.833687\n",
            "Train Epoch: 5 [22000/28539 (77%)]\tLoss: 0.794676\n",
            "Train Epoch: 5 [23000/28539 (81%)]\tLoss: 0.979521\n",
            "Train Epoch: 5 [24000/28539 (84%)]\tLoss: 1.012807\n",
            "Train Epoch: 5 [25000/28539 (88%)]\tLoss: 0.974401\n",
            "Train Epoch: 5 [26000/28539 (91%)]\tLoss: 0.916719\n",
            "Train Epoch: 5 [27000/28539 (95%)]\tLoss: 0.855383\n",
            "Train Epoch: 5 [28000/28539 (98%)]\tLoss: 0.885868\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.6727, Average CER: 0.205942 Average WER: 0.5794\n",
            "\n",
            "Train Epoch: 6 [0/28539 (0%)]\tLoss: 0.947385\n",
            "Train Epoch: 6 [1000/28539 (4%)]\tLoss: 0.925686\n",
            "Train Epoch: 6 [2000/28539 (7%)]\tLoss: 0.835680\n",
            "Train Epoch: 6 [3000/28539 (11%)]\tLoss: 0.689173\n",
            "Train Epoch: 6 [4000/28539 (14%)]\tLoss: 0.775621\n",
            "Train Epoch: 6 [5000/28539 (18%)]\tLoss: 0.792226\n",
            "Train Epoch: 6 [6000/28539 (21%)]\tLoss: 0.917954\n",
            "Train Epoch: 6 [7000/28539 (25%)]\tLoss: 0.739730\n",
            "Train Epoch: 6 [8000/28539 (28%)]\tLoss: 1.065903\n",
            "Train Epoch: 6 [9000/28539 (32%)]\tLoss: 0.834866\n",
            "Train Epoch: 6 [10000/28539 (35%)]\tLoss: 0.831534\n",
            "Train Epoch: 6 [11000/28539 (39%)]\tLoss: 0.870972\n",
            "Train Epoch: 6 [12000/28539 (42%)]\tLoss: 0.838809\n",
            "Train Epoch: 6 [13000/28539 (46%)]\tLoss: 0.848558\n",
            "Train Epoch: 6 [14000/28539 (49%)]\tLoss: 0.826617\n",
            "Train Epoch: 6 [15000/28539 (53%)]\tLoss: 0.818649\n",
            "Train Epoch: 6 [16000/28539 (56%)]\tLoss: 0.732789\n",
            "Train Epoch: 6 [17000/28539 (60%)]\tLoss: 0.783383\n",
            "Train Epoch: 6 [18000/28539 (63%)]\tLoss: 0.670558\n",
            "Train Epoch: 6 [19000/28539 (67%)]\tLoss: 0.745102\n",
            "Train Epoch: 6 [20000/28539 (70%)]\tLoss: 0.723616\n",
            "Train Epoch: 6 [21000/28539 (74%)]\tLoss: 0.750589\n",
            "Train Epoch: 6 [22000/28539 (77%)]\tLoss: 0.731420\n",
            "Train Epoch: 6 [23000/28539 (81%)]\tLoss: 0.870060\n",
            "Train Epoch: 6 [24000/28539 (84%)]\tLoss: 0.753871\n",
            "Train Epoch: 6 [25000/28539 (88%)]\tLoss: 0.681149\n",
            "Train Epoch: 6 [26000/28539 (91%)]\tLoss: 0.808485\n",
            "Train Epoch: 6 [27000/28539 (95%)]\tLoss: 0.793413\n",
            "Train Epoch: 6 [28000/28539 (98%)]\tLoss: 0.869974\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.6204, Average CER: 0.187141 Average WER: 0.5385\n",
            "\n",
            "Train Epoch: 7 [0/28539 (0%)]\tLoss: 0.814904\n",
            "Train Epoch: 7 [1000/28539 (4%)]\tLoss: 0.670384\n",
            "Train Epoch: 7 [2000/28539 (7%)]\tLoss: 0.751801\n",
            "Train Epoch: 7 [3000/28539 (11%)]\tLoss: 0.821391\n",
            "Train Epoch: 7 [4000/28539 (14%)]\tLoss: 0.696689\n",
            "Train Epoch: 7 [5000/28539 (18%)]\tLoss: 0.825674\n",
            "Train Epoch: 7 [6000/28539 (21%)]\tLoss: 0.744486\n",
            "Train Epoch: 7 [7000/28539 (25%)]\tLoss: 0.822641\n",
            "Train Epoch: 7 [8000/28539 (28%)]\tLoss: 0.734546\n",
            "Train Epoch: 7 [9000/28539 (32%)]\tLoss: 0.736383\n",
            "Train Epoch: 7 [10000/28539 (35%)]\tLoss: 0.781158\n",
            "Train Epoch: 7 [11000/28539 (39%)]\tLoss: 0.658485\n",
            "Train Epoch: 7 [12000/28539 (42%)]\tLoss: 0.932706\n",
            "Train Epoch: 7 [13000/28539 (46%)]\tLoss: 0.690144\n",
            "Train Epoch: 7 [14000/28539 (49%)]\tLoss: 0.700410\n",
            "Train Epoch: 7 [15000/28539 (53%)]\tLoss: 0.699286\n",
            "Train Epoch: 7 [16000/28539 (56%)]\tLoss: 0.887441\n",
            "Train Epoch: 7 [17000/28539 (60%)]\tLoss: 0.822956\n",
            "Train Epoch: 7 [18000/28539 (63%)]\tLoss: 0.742278\n",
            "Train Epoch: 7 [19000/28539 (67%)]\tLoss: 0.921728\n",
            "Train Epoch: 7 [20000/28539 (70%)]\tLoss: 0.614994\n",
            "Train Epoch: 7 [21000/28539 (74%)]\tLoss: 0.762344\n",
            "Train Epoch: 7 [22000/28539 (77%)]\tLoss: 0.642006\n",
            "Train Epoch: 7 [23000/28539 (81%)]\tLoss: 0.706151\n",
            "Train Epoch: 7 [24000/28539 (84%)]\tLoss: 0.686855\n",
            "Train Epoch: 7 [25000/28539 (88%)]\tLoss: 0.623298\n",
            "Train Epoch: 7 [26000/28539 (91%)]\tLoss: 0.980994\n",
            "Train Epoch: 7 [27000/28539 (95%)]\tLoss: 0.870038\n",
            "Train Epoch: 7 [28000/28539 (98%)]\tLoss: 0.785832\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.5792, Average CER: 0.173454 Average WER: 0.5086\n",
            "\n",
            "Train Epoch: 8 [0/28539 (0%)]\tLoss: 0.745246\n",
            "Train Epoch: 8 [1000/28539 (4%)]\tLoss: 0.759296\n",
            "Train Epoch: 8 [2000/28539 (7%)]\tLoss: 0.721381\n",
            "Train Epoch: 8 [3000/28539 (11%)]\tLoss: 0.630259\n",
            "Train Epoch: 8 [4000/28539 (14%)]\tLoss: 0.768148\n",
            "Train Epoch: 8 [5000/28539 (18%)]\tLoss: 0.730240\n",
            "Train Epoch: 8 [6000/28539 (21%)]\tLoss: 0.756037\n",
            "Train Epoch: 8 [7000/28539 (25%)]\tLoss: 0.938187\n",
            "Train Epoch: 8 [8000/28539 (28%)]\tLoss: 0.782329\n",
            "Train Epoch: 8 [9000/28539 (32%)]\tLoss: 0.671379\n",
            "Train Epoch: 8 [10000/28539 (35%)]\tLoss: 0.653416\n",
            "Train Epoch: 8 [11000/28539 (39%)]\tLoss: 0.665316\n",
            "Train Epoch: 8 [12000/28539 (42%)]\tLoss: 0.602302\n",
            "Train Epoch: 8 [13000/28539 (46%)]\tLoss: 0.742156\n",
            "Train Epoch: 8 [14000/28539 (49%)]\tLoss: 0.763702\n",
            "Train Epoch: 8 [15000/28539 (53%)]\tLoss: 0.686861\n",
            "Train Epoch: 8 [16000/28539 (56%)]\tLoss: 0.517872\n",
            "Train Epoch: 8 [17000/28539 (60%)]\tLoss: 0.584329\n",
            "Train Epoch: 8 [18000/28539 (63%)]\tLoss: 0.697138\n",
            "Train Epoch: 8 [19000/28539 (67%)]\tLoss: 0.707356\n",
            "Train Epoch: 8 [20000/28539 (70%)]\tLoss: 0.786635\n",
            "Train Epoch: 8 [21000/28539 (74%)]\tLoss: 0.596077\n",
            "Train Epoch: 8 [22000/28539 (77%)]\tLoss: 0.785375\n",
            "Train Epoch: 8 [23000/28539 (81%)]\tLoss: 0.782738\n",
            "Train Epoch: 8 [24000/28539 (84%)]\tLoss: 0.666541\n",
            "Train Epoch: 8 [25000/28539 (88%)]\tLoss: 0.676307\n",
            "Train Epoch: 8 [26000/28539 (91%)]\tLoss: 0.735375\n",
            "Train Epoch: 8 [27000/28539 (95%)]\tLoss: 0.745003\n",
            "Train Epoch: 8 [28000/28539 (98%)]\tLoss: 0.722013\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.5456, Average CER: 0.164474 Average WER: 0.4855\n",
            "\n",
            "Train Epoch: 9 [0/28539 (0%)]\tLoss: 0.751154\n",
            "Train Epoch: 9 [1000/28539 (4%)]\tLoss: 0.718344\n",
            "Train Epoch: 9 [2000/28539 (7%)]\tLoss: 0.576569\n",
            "Train Epoch: 9 [3000/28539 (11%)]\tLoss: 0.632036\n",
            "Train Epoch: 9 [4000/28539 (14%)]\tLoss: 0.627379\n",
            "Train Epoch: 9 [5000/28539 (18%)]\tLoss: 0.654325\n",
            "Train Epoch: 9 [6000/28539 (21%)]\tLoss: 0.706263\n",
            "Train Epoch: 9 [7000/28539 (25%)]\tLoss: 0.528171\n",
            "Train Epoch: 9 [8000/28539 (28%)]\tLoss: 0.706310\n",
            "Train Epoch: 9 [9000/28539 (32%)]\tLoss: 0.556018\n",
            "Train Epoch: 9 [10000/28539 (35%)]\tLoss: 0.648245\n",
            "Train Epoch: 9 [11000/28539 (39%)]\tLoss: 0.642569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "1Stg96IVJMP5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hhuPCF0W8EuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "goIMIIEy8EwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yw2Bz6K-8Eyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iPaGkdyd8E2O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}